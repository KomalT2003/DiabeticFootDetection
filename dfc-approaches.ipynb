{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9813651,"sourceType":"datasetVersion","datasetId":6016435}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\nimport numpy as np\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-05T14:15:10.824893Z","iopub.execute_input":"2024-11-05T14:15:10.825343Z","iopub.status.idle":"2024-11-05T14:15:10.833093Z","shell.execute_reply.started":"2024-11-05T14:15:10.825302Z","shell.execute_reply":"2024-11-05T14:15:10.831657Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"FIRST CREATING A CUSTOM CLASS FOR ONLY FRONT IMAGES","metadata":{}},{"cell_type":"code","source":"\nclass FrontFootDataset(Dataset):\n    def __init__(self, root_dir, phase, transform=None):\n        self.root_dir = os.path.join(root_dir, phase)\n        self.transform = transform\n        self.image_paths = []\n        self.labels = []\n        self.class_to_idx = {'Normal': 0, 'DFC': 1, 'Controlled': 2}\n        \n        for label in self.class_to_idx:\n            class_dir = os.path.join(self.root_dir, label, 'Front')\n            for img_name in os.listdir(class_dir):\n                if img_name.endswith(('.png', '.jpg', '.jpeg')):  # Adjust based on your image format\n                    self.image_paths.append(os.path.join(class_dir, img_name))\n                    self.labels.append(self.class_to_idx[label])\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n        label = self.labels[idx]\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T14:15:13.603193Z","iopub.execute_input":"2024-11-05T14:15:13.603653Z","iopub.status.idle":"2024-11-05T14:15:13.613866Z","shell.execute_reply.started":"2024-11-05T14:15:13.603611Z","shell.execute_reply":"2024-11-05T14:15:13.612445Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"root_dir = '/kaggle/input/diabeticfootimages/Dataset_cleaned'  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T14:15:16.713453Z","iopub.execute_input":"2024-11-05T14:15:16.713923Z","iopub.status.idle":"2024-11-05T14:15:16.720068Z","shell.execute_reply.started":"2024-11-05T14:15:16.713884Z","shell.execute_reply":"2024-11-05T14:15:16.718644Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Image transformations\ndata_transforms = {\n    'Train': transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'Validation': transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T14:15:20.401758Z","iopub.execute_input":"2024-11-05T14:15:20.402820Z","iopub.status.idle":"2024-11-05T14:15:20.412826Z","shell.execute_reply.started":"2024-11-05T14:15:20.402768Z","shell.execute_reply":"2024-11-05T14:15:20.411383Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"image_datasets = {\n    'train': FrontFootDataset(root_dir, 'Train', transform=data_transforms['Train']),\n    'val': FrontFootDataset(root_dir, 'Validation', transform=data_transforms['Validation'])\n}\ndataloaders = {\n    'train': DataLoader(image_datasets['train'], batch_size=32, shuffle=True),\n    'val': DataLoader(image_datasets['val'], batch_size=32, shuffle=False)\n}\nclass_names = list(image_datasets['train'].class_to_idx.keys())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T14:15:24.422602Z","iopub.execute_input":"2024-11-05T14:15:24.423019Z","iopub.status.idle":"2024-11-05T14:15:24.443889Z","shell.execute_reply.started":"2024-11-05T14:15:24.422980Z","shell.execute_reply":"2024-11-05T14:15:24.442649Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"# RESNET","metadata":{}},{"cell_type":"code","source":"def get_model(model_name='resnet'):\n    if model_name == 'resnet':\n        model = models.resnet50(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, len(class_names))\n    elif model_name == 'vgg':\n        model = models.vgg16(pretrained=True)\n        model.classifier[6] = nn.Linear(model.classifier[6].in_features, len(class_names))\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T14:15:52.642599Z","iopub.execute_input":"2024-11-05T14:15:52.643691Z","iopub.status.idle":"2024-11-05T14:15:52.650410Z","shell.execute_reply.started":"2024-11-05T14:15:52.643637Z","shell.execute_reply":"2024-11-05T14:15:52.649008Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"# Finetuning Resnet","metadata":{}},{"cell_type":"code","source":"def train_model(model, criterion, optimizer, num_epochs=10):\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    \n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch}/{num_epochs - 1}')\n        \n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n                \n            running_loss = 0.0\n            running_corrects = 0\n            \n            for inputs, labels in dataloaders[phase]:\n                inputs, labels = inputs.to(device), labels.to(device)\n                optimizer.zero_grad()\n                \n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n                    \n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                \n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n            \n            epoch_loss = running_loss / len(image_datasets[phase])\n            epoch_acc = running_corrects.double() / len(image_datasets[phase])\n            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T14:16:04.270267Z","iopub.execute_input":"2024-11-05T14:16:04.270686Z","iopub.status.idle":"2024-11-05T14:16:04.281993Z","shell.execute_reply.started":"2024-11-05T14:16:04.270648Z","shell.execute_reply":"2024-11-05T14:16:04.280333Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Initialize and train models\nresnet_model = get_model('resnet')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T14:16:20.808672Z","iopub.execute_input":"2024-11-05T14:16:20.809109Z","iopub.status.idle":"2024-11-05T14:16:22.444852Z","shell.execute_reply.started":"2024-11-05T14:16:20.809069Z","shell.execute_reply":"2024-11-05T14:16:22.443872Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 156MB/s] \n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"trained_resnet = train_model(resnet_model, criterion, resnet_optimizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T14:17:25.132270Z","iopub.execute_input":"2024-11-05T14:17:25.132737Z","iopub.status.idle":"2024-11-05T14:31:55.413549Z","shell.execute_reply.started":"2024-11-05T14:17:25.132695Z","shell.execute_reply":"2024-11-05T14:31:55.412336Z"}},"outputs":[{"name":"stdout","text":"Epoch 0/9\ntrain Loss: 0.9411 Acc: 0.5055\nval Loss: 0.9723 Acc: 0.4737\nEpoch 1/9\ntrain Loss: 0.7856 Acc: 0.6593\nval Loss: 0.9611 Acc: 0.6842\nEpoch 2/9\ntrain Loss: 0.6930 Acc: 0.7418\nval Loss: 0.8697 Acc: 0.6316\nEpoch 3/9\ntrain Loss: 0.5647 Acc: 0.8462\nval Loss: 0.7050 Acc: 0.6316\nEpoch 4/9\ntrain Loss: 0.4538 Acc: 0.8846\nval Loss: 0.5741 Acc: 0.6842\nEpoch 5/9\ntrain Loss: 0.3904 Acc: 0.8956\nval Loss: 0.4605 Acc: 0.7368\nEpoch 6/9\ntrain Loss: 0.2869 Acc: 0.9286\nval Loss: 0.3844 Acc: 0.7895\nEpoch 7/9\ntrain Loss: 0.2181 Acc: 0.9451\nval Loss: 0.3119 Acc: 0.8421\nEpoch 8/9\ntrain Loss: 0.1825 Acc: 0.9780\nval Loss: 0.2563 Acc: 0.9474\nEpoch 9/9\ntrain Loss: 0.1401 Acc: 0.9725\nval Loss: 0.1911 Acc: 1.0000\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\nresnet_optimizer = optim.SGD(resnet_model.parameters(), lr=0.001, momentum=0.9)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T14:16:41.503200Z","iopub.execute_input":"2024-11-05T14:16:41.503681Z","iopub.status.idle":"2024-11-05T14:16:41.510884Z","shell.execute_reply.started":"2024-11-05T14:16:41.503628Z","shell.execute_reply":"2024-11-05T14:16:41.509595Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Evaluation and metrics\ndef evaluate_model(model):\n    # Define device within the function\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    model.eval()\n    \n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for inputs, labels in dataloaders['val']:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    accuracy = accuracy_score(all_labels, all_preds)\n    report = classification_report(all_labels, all_preds, target_names=class_names)\n    conf_matrix = confusion_matrix(all_labels, all_preds)\n    \n    print(f'Accuracy: {accuracy:.4f}')\n    print('Classification Report:')\n    print(report)\n    print('Confusion Matrix:')\n    print(conf_matrix)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T14:35:25.425735Z","iopub.execute_input":"2024-11-05T14:35:25.426232Z","iopub.status.idle":"2024-11-05T14:35:25.436084Z","shell.execute_reply.started":"2024-11-05T14:35:25.426189Z","shell.execute_reply":"2024-11-05T14:35:25.434750Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# Now evaluate the model\nevaluate_model(trained_resnet)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T14:35:31.399730Z","iopub.execute_input":"2024-11-05T14:35:31.400197Z","iopub.status.idle":"2024-11-05T14:35:35.374798Z","shell.execute_reply.started":"2024-11-05T14:35:31.400153Z","shell.execute_reply":"2024-11-05T14:35:35.373649Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 1.0000\nClassification Report:\n              precision    recall  f1-score   support\n\n      Normal       1.00      1.00      1.00         3\n         DFC       1.00      1.00      1.00         9\n  Controlled       1.00      1.00      1.00         7\n\n    accuracy                           1.00        19\n   macro avg       1.00      1.00      1.00        19\nweighted avg       1.00      1.00      1.00        19\n\nConfusion Matrix:\n[[3 0 0]\n [0 9 0]\n [0 0 7]]\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"## CREATE A TEST SET USING IMAGE-2-IMAGE STABLE DIFFUSION MODELS","metadata":{}},{"cell_type":"code","source":"from diffusers import StableDiffusionImg2ImgPipeline\nimport torch\nfrom PIL import Image\nimport os\n\n# Load the Stable Diffusion Image-to-Image pipeline\npipe = StableDiffusionImg2ImgPipeline.from_pretrained(\"CompVis/stable-diffusion-v-1-4\", torch_dtype=torch.float16)\npipe.to(\"cuda\")\n\n# Define paths\ntrain_images_dir = '/path/to/your/train/images'  # Folder containing training images\noutput_dir = ''\nos.makedirs(output_dir, exist_ok=True)\n\n# Prompts corresponding to the classes\nclass_prompts = {\n    \"normal\": \"a normal foot\",\n    \"dfc\": \"a diabetic foot image\",\n    \"controlled\": \"a controlled diabetic foot image\"\n}\n\n#\nfor class_name, prompt in class_prompts.items():\n    class_path = os.path.join(train_images_dir, class_name)\n    output_class_path = os.path.join(output_dir, class_name)\n    os.makedirs(output_class_path, exist_ok=True)\n    \n    for img_name in os.listdir(class_path):\n        img_path = os.path.join(class_path, img_name)\n        \n        # Open and preprocess the image\n        init_image = Image.open(img_path).convert(\"RGB\").resize((512, 512))  # Resize to model's expected input size\n\n        # Generate a specified number of variations per image\n        for i in range(5):\n            # Generate a variation based on the prompt and initial image\n            generated_image = pipe(prompt=prompt, init_image=init_image, strength=0.75).images[0]\n            \n            # Save the generated image\n            gen_img_name = f\"{os.path.splitext(img_name)[0]}_gen_{i}.png\"\n            generated_image.save(os.path.join(output_class_path, gen_img_name))\n\nprint(\"Image generation complete.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DATA AUGMENTATION","metadata":{}},{"cell_type":"code","source":"\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomRotation(15),  # Randomly rotate images by up to 15 degrees\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Random color changes\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}